[{"title":"神经网络和深度学习","date":"2020-07-08T04:42:32.284Z","path":"2020/07/08/神经网络和深度学习/","content":"<p>建立深度学习的步骤及其注意事项</p>\n<a id=\"more\"></a>\n<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>深度学习，机器学习，简单而言就是计算机基于大量的实例而得出的判断，这和人的经验主义表达的意思非常像。我今天吃A家的火锅，结果回家之后疯狂拉肚子，我就有经验了，我以后都不会去A家去吃火锅。计算机比较笨，不会像我一样，只要一次就得出结论，所以想要计算机像我一样聪明，就需要对计算机进行训练。也就是深度学习和机器学习。比如给计算机大量的猫片和非猫片，告诉计算机什么是猫片，什么不是猫片，这样计算机在之后见到猫片后就知道。哦，这是猫片！</p>\n<h2 id=\"1-建立模型\"><a href=\"#1-建立模型\" class=\"headerlink\" title=\"(1)建立模型\"></a>(1)建立模型</h2><p>这里我只学习了logistic线性回归的模型，每一个模型都拥有其自己独有的计算公式，比如logistics模型的公式就如下所示：</p>\n<script type=\"math/tex; mode=display\">\nlogical = Z = w^tx+b</script><p>很明显上面这个公式得出的结果不可能一定是一个介于0-1之间的数，而我们所希望的，是计算机给出这张图片是猫片的概率有多大，所以我们在这里需要对logistics模型得出的结果Z进行归一化，这样各张图片的Z之间就具有了更好的可比性。归一化使用的是sigmoid函数，其方法如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\sigma (A)=\\frac{1}{1+e^{-z}}</script><p>进行完该步骤之后，计算机每次得出的结果都是一个概率值。</p>\n<h2 id=\"2-寻找公式中的影响因素\"><a href=\"#2-寻找公式中的影响因素\" class=\"headerlink\" title=\"(2)寻找公式中的影响因素\"></a>(2)寻找公式中的影响因素</h2><p>在第一步选择好将要使用的模型后，就可以从模型的对应计算公式中找到相应的影响因素，不如在logistics模型中，主要的影响因子有“ω”和“b”。而x则为公式的输入对象。Z为公式的输出对象。</p>\n<h2 id=\"3-正向推导公式\"><a href=\"#3-正向推导公式\" class=\"headerlink\" title=\"(3)正向推导公式\"></a>(3)正向推导公式</h2><p>第二步找到影响因子后就要正向推导该模型公式。即怎么从影响因子计算出Z。可以看到，是先使用ω和x做乘法，再加上一个b，也就是说公式可以如下所示这样推导：</p>\n<script type=\"math/tex; mode=display\">\nu = w^t*x</script><script type=\"math/tex; mode=display\">\ny = u+b</script><script type=\"math/tex; mode=display\">\nZ = u+y</script><p>从正向推导中可以看出，除了ω和b之外，还有两个中间变量u和y。这些变量共同作用，计算出了Z。这里补充说明的是，x可以是一个矩阵形式的输入，而ω同样也可以以一个矩阵的形式进行输入。</p>\n<h2 id=\"4-建立训练集\"><a href=\"#4-建立训练集\" class=\"headerlink\" title=\"(4)建立训练集\"></a>(4)建立训练集</h2><p>在logistics模型的这个公式中，我们只需要知道ω和b的取值，就可以使用这个模型来进行结果的预测。比如预测一张输入的图片是不是一张“猫猫”的照片。而这个ω和b的取值由电脑给出。也就是说只要给电脑大量的图片，并且表明图片中存在“猫猫”的概率有多大。即可以让电脑自行判断ω和b取什么值时。得出的预测结果是最准确的，这就是机器学习。而上面给出大量图片的操作就是建立一个训练集的操作。<u>（至于这里为什么给出的训练集中一张图片是不是猫片用概率来表示而不是用01，这里我也不懂，等待后期更新。）</u></p>\n<p>基于上面这个训练集的定义，我们就可以假定训练集中一张图片是猫片的概率为Y。假定计算机通过机器学习之后给出的相同的这张图片是猫片的概率为A。这时，我们就可以定义出训练的损失函数，具体公式如下所示：</p>\n<script type=\"math/tex; mode=display\">\nL(Y,A)=-(Y*ln(A)+(1-Y)ln(1-A))</script><p>当然，上面这个公式只是针对训练集中一张图片的预测准确性进行的判断，由此可以定义出针对于真个训练集的成本函数Cost（也叫<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数），具体公式如下所示：</p>\n<script type=\"math/tex; mode=display\">\nJ(\\omega ,b)=\\frac{1}{m}*\\sum_{i=1}^{m}L(Y^{i},A^{i})</script><p>把所有图片的预测准确性判断加起来，可以看出，在<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数中只有ω和b参数对其造成了影响。我们目前的目的就是要使得<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数的取值尽可能的小。其值越小，说明机器学习效果越好，预测的越准确，所以我们要分析ω和b参数，也就是第5步要做的事情。</p>\n<h2 id=\"5-对训练集中的每一个数据都进行反向推导\"><a href=\"#5-对训练集中的每一个数据都进行反向推导\" class=\"headerlink\" title=\"(5)对训练集中的每一个数据都进行反向推导\"></a>(5)对训练集中的每一个数据都进行反向推导</h2><p>在前面的第3步中，我们进行的正向推导，现在我们需要进行反向推导。至于为什么要进行反向推导，因为计算机拿到的训练集都是有“答案”的训练集了。计算机需要根据这个“答案”去分析，各个影响因子是如何相互作用后产生该答案的，这些相互影响的影响因子之间的取值是多少。下面就让我们开始反向推导：</p>\n<script type=\"math/tex; mode=display\">\nlogical = Z = w^tx+b</script><p>这是原公式。这里要再说明一下，logistics模型一般使用梯度下降法来寻找ω和b的最佳取值,也就是让ω和b沿着斜率最大的方向进行下降，尽量缩短机器学习所需要的的时间，病提高准确率。具体公式如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\omega=\\omega -\\alpha *\\frac{\\partial J(\\omega ,b)) }{\\partial \\omega }</script><script type=\"math/tex; mode=display\">\nb=b -\\alpha *\\frac{\\partial J(\\omega ,b)) }{\\partial b }</script><p>α是机器的学习效率，即用<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数对ω和b求偏导。</p>\n<p>当然，从这里可以看出来，如果直接用<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数来对ω或者对b求偏导数，那么难度还是很大的，因为从ω，b到<img src=\"https://latex.codecogs.com/gif.latex?J\" alt=\"This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.\">函数经历了一系列的变换，这也是反向推导存在的意义。我们可以根据求导的法则一步一步进行反向的求导。</p>\n<p>1.由于<strong>J</strong>函数本身由多个L函数加和而来，所以对<strong>J</strong>函数求导就是分别对每一个L函数求导，L函数的求导过程如下所示：</p>\n<script type=\"math/tex; mode=display\">\n-(Y*ln(A)+(1-Y)ln(1-A))——>-\\frac{Y}{A}+\\frac{1-Y}{1-A}——>\\frac{dL}{dA}</script><p>即对y进行求导。</p>\n<p>2.下面就可以开始对A函数，也就是有sigmoid函数得出的函数进行求导，即dA/dZ。其计算公式如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{1}{1+e^{-z}}——>\\frac{e^{-z}}{(1+e^{-z})^{2}}</script><p>同时，又有如下公式：</p>\n<script type=\"math/tex; mode=display\">\n\\sigma (A)=\\frac{1}{1+e^{-z}}</script><p>所以，用上式替换上上式中的值，可得：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{e^{-z}}{(1+e^{-z})^{2}}——>A(1-A)——>\\frac{dA}{dZ}</script><p>所以dL/dZ如下所示为</p>\n<script type=\"math/tex; mode=display\">\nA-Y</script><p>下面就要来计算dZ/dω，dZ/db和dZ/dx了。这里假设ω,x只有一个。当然，ω和x可以是矩阵形式，也就是多个。具体公式如下所示：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{dZ}{d\\omega }=xdZ</script><script type=\"math/tex; mode=display\">\ndb=dZ</script><p>而这里的dZ可以理解为dZ=A-Y。</p>\n<p>所以就算出了梯度下降法的各个影响参数：</p>\n<script type=\"math/tex; mode=display\">\n\\omega=\\omega -\\alpha dZ=\\omega-\\alpha*(A-Y)</script><script type=\"math/tex; mode=display\">\nb=b -\\alpha dZ=b -\\alpha*(A-Y)</script><h2 id=\"6-把各个参数结果相加后计算出全局参数值\"><a href=\"#6-把各个参数结果相加后计算出全局参数值\" class=\"headerlink\" title=\"(6)把各个参数结果相加后计算出全局参数值\"></a>(6)把各个参数结果相加后计算出全局参数值</h2><p>根据第5步的各个公式即可算出ω和b的最大斜率方向，而在第3步中已经提到过，ω和x可以是以矩阵的形式存在的，也就是说ω*x可以看成是两个矩阵相乘的形式，则会出现下列的公式：</p>\n<script type=\"math/tex; mode=display\">\n\\omega *x=\\begin{pmatrix}\n\\omega _{1} & \\omega _{2}\n\\end{pmatrix}*\\begin{pmatrix}\nx_{1}\\\\ x_{2}\n\\end{pmatrix}=\\omega_{1}*x_{1}+\\omega_{2}*x_{2}</script><p>可以从这个公式中看出，ω有两个，x也有两个，所以都要依据第5步的方法计算之后，分别加和求出其平均值，最后再套用公式如下来使用梯度下降法</p>\n<script type=\"math/tex; mode=display\">\n\\omega_{i}^{}=\\omega_{i}^{} -\\alpha dZ</script><script type=\"math/tex; mode=display\">\nb=b -\\alpha dZ</script><h2 id=\"7-把上述步骤拓展到m个训练集样本中\"><a href=\"#7-把上述步骤拓展到m个训练集样本中\" class=\"headerlink\" title=\"(7)把上述步骤拓展到m个训练集样本中\"></a>(7)把上述步骤拓展到m个训练集样本中</h2><p>在上面的6个步骤后，我们完成了一个训练集中一个样本各个参数值得计算，现在我们要完成一个训练集中剩余样本的计算。这里我直接给出直观的代码来帮助理解：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">double</span> J = <span class=\"number\">0</span> , dw1 = <span class=\"number\">0</span> , dw2 = <span class=\"number\">0</span> , db = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i =<span class=\"number\">0</span>;i&lt;m;m++)&#123;<span class=\"comment\">//m为训练集中样本个数</span></span><br><span class=\"line\">    Z(i) = W*x[i]=b;</span><br><span class=\"line\">    A[i] = sigmoid(Z[i]);</span><br><span class=\"line\">    J += -[Y[i]*ln(a[i])+(<span class=\"number\">1</span>-Y[i]*ln(<span class=\"number\">1</span>-a[i]))];</span><br><span class=\"line\">    dZ[i] = A[i] - Y[i];</span><br><span class=\"line\">    dw1 += x1[i]*dZ[i];</span><br><span class=\"line\">    dw2 += x2[i]*dZ[i];</span><br><span class=\"line\">    db += dZ[i];</span><br><span class=\"line\">    J = j/m;</span><br><span class=\"line\">    dw1 = dw1/m;</span><br><span class=\"line\">    dw2 = dw2/m;</span><br><span class=\"line\">    db = db/m;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>经历上述步骤之后，就完成了梯度下降法的一次迭代。这时，ω和b的值为dw1,dw2,db所取代。如果要进行多次迭代，只需要在这个程序外围在加入一圈for循环即可。</p>\n<p>一直训练到ω和b的值不在发生改变时，深度学习就完成了，此时的ω和b就是最佳的参数。</p>\n<h2 id=\"8-疑惑解答\"><a href=\"#8-疑惑解答\" class=\"headerlink\" title=\"(8)疑惑解答\"></a>(8)疑惑解答</h2><p>1，在一开始进行深度学习时，会初始化ω和b。可以任意指定ω和b的值，最后通过迭代的方法来找到ω和b的最优值。</p>\n<p>2，每次外层循环完成之后，会用求得的dw1，dw2…和db的值来更新ω和b的值。直至ω和b的值不在发生变化时，则整个深度学习过程结束。</p>\n<h2 id=\"9-改进方向\"><a href=\"#9-改进方向\" class=\"headerlink\" title=\"(9)改进方向\"></a>(9)改进方向</h2><p>尽量使用矩阵计算而不是for循环来计算。ω和x是两个矩阵，完全可以使用矩阵的方法来完成计算。</p>\n","excerpt":"建立深度学习的步骤及其注意事项","categories":[],"tags":[]},{"title":"Hello World","date":"2020-07-07T14:16:34.703Z","path":"2020/07/07/hello-world/","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/Crimson-pro/Crimson-pro.github.io\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><p>why i cant say CN?</p>\n<h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://github.com/Crimson-pro/Crimson-pro.github.io\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","excerpt":"","categories":[],"tags":[]}]